# Browser Voice Agent

> A fully browser-native voice agent powered by local LLMs, text-to-speech, and speech recognition. No server required â€” everything runs on your device.

![Desktop Dashboard](docs/screenshots/desktop-dashboard.png)

---

## What is this?

A real-time voice conversation agent that runs entirely in your browser:

1. **You speak** â€” Web Speech API transcribes your voice
2. **Agent thinks** â€” A local LLM (via WebGPU) classifies your intent and generates a response
3. **Agent speaks back** â€” Text-to-speech reads the response aloud
4. **Loop repeats** â€” The agent adapts its behavior based on your reactions

No API keys. No cloud servers. No data leaves your device.

---

## Features

- **Local LLM inference** via WebGPU â€” choose from 15+ models (0.4GB to 5.5GB)
- **Text-to-Speech** â€” VITS neural voices on desktop, native SpeechSynthesis on mobile
- **Speech-to-Text** â€” Web Speech API with continuous listening and silence detection
- **FSM architecture** â€” 8-stage state machine: Listen â†’ Detect â†’ Classify â†’ Respond â†’ Speak â†’ Observe â†’ Adapt â†’ Loop
- **Streaming TTS** â€” sentences are spoken as the LLM generates them, not after
- **Dual voice mode** â€” internal monologue (soft female) + response (British voice) on desktop
- **Adaptive bias system** â€” the agent adjusts verbosity, confidence thresholds, and response speed over time
- **Full dashboard** â€” stage pipeline, decision trace, prompt inspection, bias sliders, A/B model comparison

---

## Mobile / iPhone

<img src="docs/screenshots/mobile-empty-state.png" alt="Mobile View" width="300" />

The app works on **iPhone Chrome** (iOS 18+) with automatic optimizations:

| Feature | Desktop (Chrome) | iPhone (Chrome) |
|---------|------------------|-----------------|
| LLM | WebGPU | WebGPU |
| TTS | VITS neural voices | Native SpeechSynthesis (auto-fallback) |
| STT | Web Speech API | Web Speech API |
| Concurrency | 2 parallel TTS chunks | 1 (reduced for memory) |
| Think tags | Spoken as monologue | Skipped (response only) |

**Safari is not supported** â€” it lacks the required WebGPU and Web Speech API support. The app shows a clear message if opened in Safari.

### Mobile optimizations

- ONNX/VITS skipped entirely on iOS â€” native `speechSynthesis` used from the start
- Audio unlock on first tap (iOS requires user gesture for speech output)
- AudioContext suspended during TTS to prevent audio session conflicts
- Backpressure on TTS queue (max 3 pending chunks) to prevent memory exhaustion

---

## Quick Start

```bash
git clone https://github.com/davidbmar/browser-voice-agent-with-TTS-STT-and-OpenSourceLLMs-demo.git
cd browser-voice-agent-with-TTS-STT-and-OpenSourceLLMs-demo
npm install
npm run dev
```

Open **http://localhost:5173** in Chrome. The app auto-loads the Qwen3 0.6B model on startup.

### Requirements

- **Chrome 113+** or **Edge 113+** (WebGPU required)
- A GPU with at least 1GB VRAM for the smallest models
- Microphone access for speech recognition

---

## ğŸ“ Project Memory System

This repo uses a **Traceable Project Memory** system to make every coding session and decision searchable with citations.

### Important Files

- **`CLAUDE.md`** â€” Instructions for AI assistants on how to use the Project Memory system
- **`docs/project-memory/`** â€” Session logs, ADRs, runbooks, architecture docs

### How It Works

Every coding session gets a Session ID: `S-YYYY-MM-DD-HHMM-<slug>`

Every commit must include the Session ID: `[SessionID] description`

Example:
```bash
git commit -m "[S-2026-02-08-1500-add-auth] Add JWT authentication"
```

### Getting Started with Project Memory

1. Read `docs/project-memory/index.md` for full documentation
2. Copy `docs/project-memory/sessions/_template.md` to start a new session
3. Make commits with Session ID prefixes
4. Update session docs with changes and decisions
5. Create ADRs for significant architectural decisions

**When working with AI assistants (Claude, etc):** They automatically read `CLAUDE.md` and will enforce the Project Memory system for you.

---

## Deploy to AWS

The included `deploy.sh` script handles S3 + CloudFront deployment with proper COOP/COEP headers (required for SharedArrayBuffer):

```bash
# First time: set up S3 bucket, CloudFront distribution, and response headers
./deploy.sh --setup

# Subsequent deploys: build, sync to S3, invalidate cache
./deploy.sh
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FSM Loop                          â”‚
â”‚                                                     â”‚
â”‚  IDLE â†’ LISTENING â†’ SIGNAL_DETECT â†’ CLASSIFY        â”‚
â”‚    â†‘                                    â†“           â”‚
â”‚  UPDATE_BIAS â† FEEDBACK_OBSERVE â† SPEAK â† RESPOND  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Stage | What happens |
|-------|-------------|
| **LISTENING** | Mic active, Web Speech API transcribes in real-time |
| **SIGNAL_DETECT** | VAD monitors audio levels, detects silence for turn-end |
| **CLASSIFY** | Intent classification (rule-based or LLM) |
| **MICRO_RESPONSE** | LLM generates response with streaming token output |
| **SPEAK** | TTS speaks the response (sentences streamed as generated) |
| **FEEDBACK_OBSERVE** | Watches for user reaction (interruption, silence, acknowledgement) |
| **UPDATE_BIAS** | Adjusts system parameters based on observed reaction |

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Framework | React 19 + TypeScript |
| Build | Vite 7 |
| Styling | Tailwind CSS v4 |
| UI Components | shadcn/ui |
| LLM Runtime | [WebLLM](https://github.com/mlc-ai/web-llm) (WebGPU) |
| TTS (Desktop) | [vits-web](https://github.com/diffusionstudio/vits-web) (ONNX/VITS) |
| TTS (Mobile) | Native `SpeechSynthesis` API |
| STT | Web Speech API (`SpeechRecognition`) |
| Hosting | S3 + CloudFront |

---

## Project Structure

```
src/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ loop-controller.ts   # Core FSM â€” state machine + event dispatch
â”‚   â”œâ”€â”€ loop-types.ts        # State, events, model catalog
â”‚   â”œâ”€â”€ audio-listener.ts    # Mic + speech recognition + audio levels
â”‚   â”œâ”€â”€ tts-speaker.ts       # TTS engine (VITS + native fallback)
â”‚   â”œâ”€â”€ llm-engine.ts        # WebLLM wrapper for local inference
â”‚   â”œâ”€â”€ prompt-templates.ts  # Classification + response prompts
â”‚   â”œâ”€â”€ decision-trace.ts    # Event log for debugging
â”‚   â””â”€â”€ bias-store.ts        # Adaptive parameter system
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ use-loop.ts          # React hook wrapping LoopController
â”‚   â””â”€â”€ use-mobile.ts        # Viewport-based mobile detection
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ layout/              # Desktop + mobile layouts
â”‚   â”œâ”€â”€ loop/                # Stage diagram, controls
â”‚   â”œâ”€â”€ model/               # Model selector, toggles, A/B panel
â”‚   â”œâ”€â”€ state/               # Internal state display, bias sliders
â”‚   â”œâ”€â”€ prompts/             # Prompt inspection panel
â”‚   â”œâ”€â”€ trace/               # Decision trace viewer
â”‚   â”œâ”€â”€ history/             # Conversation history timeline
â”‚   â”œâ”€â”€ capabilities/        # Browser capability detection
â”‚   â””â”€â”€ ui/                  # shadcn/ui primitives
â””â”€â”€ main.tsx
```

---

## License

MIT
