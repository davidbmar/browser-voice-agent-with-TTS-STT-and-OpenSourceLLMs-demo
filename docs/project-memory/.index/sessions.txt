=== S-2026-02-08-1400-bootstrap-memory ===
# Session

Session-ID: S-2026-02-08-1400-bootstrap-memory
Title: Bootstrap project memory system
Date: 2026-02-08
Author: dmar

## Goal

Bootstrap the Project Memory system so every coding session and decision is searchable with citations.

## Context

Starting a new repo. Need a way to track:
- Why code changed
- What was decided and when
- How to find context for any commit

## Plan

1. Create directory structure for sessions, ADRs, runbooks, architecture
2. Create templates for sessions and ADRs
3. Document Session ID format and commit message standard
4. Create example files showing how it works
5. Update CLAUDE.md so AI knows how to use the system
6. Update README.md to point to CLAUDE.md

## Changes Made

- Created `docs/project-memory/` structure
- Created session and ADR templates
- Created example session (this file)
- Created example ADR for decision tracking
- Created PR template requiring Session IDs
- Documented Session ID format: `S-YYYY-MM-DD-HHMM-<slug>`
- Established commit message format: `[SessionID] description`

## Decisions Made

**Session ID Format**: Chose `S-YYYY-MM-DD-HHMM-<slug>` because:
- Sortable by time
- Human readable
- Unique enough for concurrent sessions
- Easy to grep in commits

**Commit Prefix Requirement**: Every commit must include `[SessionID]` because:
- Makes git log searchable
- Links commits to session context
- Enables tracing from code to reasoning

## Open Questions

None. System is ready to use.

## Links

Commits:
- (This is the bootstrap session)

PRs:
- (Will be linked when PR created)

ADRs:
- ADR-0001 - Session-based commit tracking

=== S-2026-02-08-1400-listener-ui-mute ===
# Session

Session-ID: S-2026-02-08-1400-listener-ui-mute
Title: Add listener visibility, collapsible panels, and audio mute
Date: 2026-02-08
Author: Claude + dmar

## Goal

Add visibility into listener pause/resume, make all panels collapsible, add audio mute toggle, add build number, and add comprehensive test suite.

## Context

After implementing the always-on listener pipeline, the UI didn't show when the listener was paused for echo cancellation. The stage diagram showed LISTENING in the sequential pipeline (but it's actually parallel). Some panels couldn't be collapsed. There was no way to mute audio output. No test infrastructure existed.

## Plan

1. Expose listenerPaused/audioMuted in LoopState
2. Two-row stage diagram (listener status + processing pipeline)
3. Make InternalStatePanel and BiasSliders collapsible
4. Audio mute toggle in ModelToggle
5. Add vitest infrastructure and comprehensive test suite
6. Build number (datetime stamp) in header
7. Speech event logging panel

## Changes Made

### Test Infrastructure
- Added vitest.config.ts, 12 test files with 208 tests covering all core modules
- Added speech-event-log.ts (event logging), format-time.ts (timezone-aware formatting)

### Listener Visibility
- Added listenerPaused and audioMuted to LoopState
- Wired onStateChange callback from AudioListener into LoopController
- Two-row stage diagram: Row 1 = listener status, Row 2 = processing pipeline

### Collapsible Panels
- InternalStatePanel: wrapped in Card + Collapsible
- BiasSliders: wrapped in Card + Collapsible

### Audio Mute
- TTSSpeaker: added muted field, setMuted(), isMuted(), applied in 4 playback paths
- ModelToggle: added Audio Output switch
- LoopController: added setAudioMuted() method
- useLoop hook: exposed setAudioMuted callback

### Build Number
- vite.config.ts: BUILD_NUMBER define (YYYYMMDDHHMMSS)
- vite-env.d.ts: TypeScript declaration
- App.tsx: displayed in header

### Speech Event Panel
- SpeechEventPanel component for real-time speech event monitoring
- SpeechEventLog class for structured event storage
- Wired into App.tsx

## Decisions Made

- Listener pause during SPEAK is unavoidable (echo cancellation) — made visible rather than eliminated
- Used VITS silence as volume=0 rather than skip to preserve pipeline timing
- Two-row stage diagram separates parallel concerns (listener vs processing)

## Open Questions

- None

## Links

Commits:
- (see git log)

PRs:
- N/A

ADRs:
- N/A

=== S-2026-02-08-1430-migrate-project-memory ===
# Session

Session-ID: S-2026-02-08-1430-migrate-project-memory
Title: Migrate project memory to voice agent repo
Date: 2026-02-08
Author: dmar

## Goal

Migrate the Traceable Project Memory system from the bootstrap repo into the browser_question_loop project and test it.

## Context

The browser_question_loop project is an active voice agent application with ongoing development. Need a system to track sessions, decisions, and commits so future changes are searchable and explainable with citations.

## Plan

1. Copy the entire Project Memory system from traceable-searchable-adr-memory-index
2. Migrate docs/project-memory/ directory structure
3. Copy CLAUDE.md so AI assistants know how to use the system
4. Copy .github/PULL_REQUEST_TEMPLATE.md
5. Update README.md to document the Project Memory system
6. Create a test session (this document) to demonstrate the workflow
7. Make a commit using the Session ID format to test it works

## Changes Made

- Copied `docs/project-memory/` directory with all templates and examples
- Copied `CLAUDE.md` with AI instructions
- Copied `.github/PULL_REQUEST_TEMPLATE.md`
- Updated `README.md` to add Project Memory System section
- Created this session document: `S-2026-02-08-1430-migrate-project-memory.md`

## Decisions Made

**Keep existing docs/screenshots/ directory**: The project already has a docs/ directory for screenshots. Project Memory lives alongside it in docs/project-memory/.

**No changes to existing workflow**: The Project Memory system is additive. Existing development continues normally, but now with Session IDs in commits and session documentation.

## Open Questions

None. Ready to test with a commit.

## Links

Commits:
- `fe10763` [S-2026-02-08-1430-migrate-project-memory] Add traceable project memory system

PRs:
- (Will be linked when PR created)

ADRs:
- See ADR-0001 in source repo for rationale on Session ID format

=== S-2026-02-08-1500-collapse-interims ===
# Session

Session-ID: S-2026-02-08-1500-collapse-interims
Title: Collapse consecutive interim speech events
Date: 2026-02-08
Author: Claude

## Goal

Collapse consecutive INTERIM speech events into a single row in the Speech Events panel to reduce noise.

## Context

The Speech Events panel shows every individual INTERIM event as a separate row, creating 15-20 lines for a single sentence. Users want a cleaner view with collapsed interims showing time range and count.

## Plan

1. Add `formatTimeRange()` to `format-time.ts`
2. Add `groupEvents()` helper and `DisplayItem` types to `speech-event-panel.tsx`
3. Update rendering to use grouped display items
4. Add tests for `formatTimeRange`

## Changes Made

- `src/lib/format-time.ts`: Added `formatTimeRange(startMs, endMs, tz)` function
- `src/components/trace/speech-event-panel.tsx`: Added grouping types/logic, updated render to collapse consecutive interims
- `src/lib/__tests__/format-time.test.ts`: Added tests for `formatTimeRange`

## Decisions Made

- Grouping is UI-only (rendering layer), SpeechEventLog unchanged
- Time range format: `HH:MM:SS.mmm-SS.mmm` for ranges > 100ms, single timestamp otherwise
- Show count of collapsed interims in parentheses at end of text

## Open Questions

None.

## Links

Commits:
- (pending)

=== S-2026-02-08-1500-fix-silence-counter ===
# Session

Session-ID: S-2026-02-08-1500-fix-silence-counter
Title: Fix premature turn-end detection clipping speech
Date: 2026-02-08
Author: Claude + dmar

## Goal

Fix premature turn-end detection that causes speech to be clipped to just the first word (e.g., "what time is it?" only captures "what").

## Context

The silence-based turn detection in `updateVADFromLevel()` (loop-controller.ts:406-420) fires a synthetic `TURN_END` when `silenceDurationMs >= silenceThresholdMs` (1500ms default) during SIGNAL_DETECT with interim text. However, `silenceDurationMs` accumulates during long silences between utterances and is never reset when new speech arrives via SpeechRecognition interim results. This causes TURN_END to fire within ~200ms of the first word, using whatever partial text is available.

Evidence from Speech Events panel:
- `FINAL "what time" (0.70)` fires 213ms after first interim `"what"` — 0.70 is the hardcoded confidence from silence-based detection
- Full sentence `"what time is it" (0.97)` arrives later during MICRO_RESPONSE, gets queued, but discarded as stale (>10s old)

## Plan

1. Reset `silenceDurationMs` and `silenceStartTime` when transitioning LISTENING → SIGNAL_DETECT
2. Add tests verifying the reset behavior
3. Deploy and verify

## Changes Made

- `src/lib/loop-controller.ts` — Reset silence counter on LISTENING→SIGNAL_DETECT transition
- `src/lib/__tests__/loop-controller.test.ts` — Add test for silence reset on signal detect

## Decisions Made

- Reset silence counter at the LISTENING→SIGNAL_DETECT transition rather than modifying `updateVADFromLevel()`. This is the most targeted fix — the problem is that old silence data persists into a new speech detection window.

## Open Questions

- Should we also reset when `isSpeaking` flips to true in `updateVADFromLevel`? Currently line 392 does `silenceDurationMs = 0` but only when audio level crosses threshold. SpeechRecognition can start producing text before audio level detection catches up.

## Links

Commits:
- `b1c92c7` [S-2026-02-08-1400-listener-ui-mute] (fix included in this commit)

PRs:
- N/A

ADRs:
- N/A

=== S-2026-02-08-1515-fix-model-switch-cache ===
# Session

Session-ID: S-2026-02-08-1515-fix-model-switch-cache
Title: Fix model switching failure from full Cache Storage
Date: 2026-02-08
Author: Claude

## Goal

Fix model switching failure caused by browser Cache Storage being full from previous model weights.

## Context

When switching models after one is already loaded, the user sees "Model load failed: Failed to execute 'add' on 'Cache': Unexpected internal error." This is because `unloadModel()` frees GPU memory but doesn't clear the Cache Storage where WebLLM stores downloaded weights. The new model download then fails due to insufficient cache space.

## Plan

1. Delete old model from Cache Storage before loading new model (llm-engine.ts)
2. Reset controller state on load failure (loop-controller.ts)
3. Clear pendingModelId on error in App.tsx

## Changes Made

- `src/lib/llm-engine.ts`: Call `deleteModelAllInfoInCache(previousModelId)` before loading new model
- `src/lib/loop-controller.ts`: Wrap `llmEngine.loadModel()` in try/catch, reset modelConfig state on failure
- `src/App.tsx`: Clear `pendingModelId` when model load fails

## Decisions Made

- Cache cleanup is best-effort (try/catch with ignored error) — if it fails, the main load might still succeed
- Old model cache is only deleted when switching to a *different* model, not on reload of the same model

## Open Questions

None.

## Links

Commits:
- (pending)

=== S-2026-02-08-1600-graceful-error-handling ===
# Session

Session-ID: S-2026-02-08-1600-graceful-error-handling
Title: Add graceful error handling for model failures
Date: 2026-02-08
Author: Claude

## Goal

Add graceful error handling for model load failures (quota exceeded) and generation failures (Tokenizer* WASM crash). Pre-flight storage check, auto-unload on fatal errors, user-friendly messages, error dismissal.

## Context

Users see raw error strings like "Model load failed: Quota exceeded" and "Generation error: Cannot pass deleted object as a pointer of type Tokenizer*". The Tokenizer error leaves the engine in a broken state where `isLoaded()` returns true but every generation fails. No pre-flight storage check exists.

## Plan

1. Add `estimateStorage()` static method to LLMEngine
2. Auto-unload engine on fatal WASM/Tokenizer errors during generate
3. User-friendly error messages via `friendlyError()` in controller
4. Clear `state.error` when entering LISTENING stage
5. Show "low storage" warning in model selector
6. Add error dismiss button in App.tsx
7. Tests for all changes

## Changes Made

1. **`src/lib/llm-engine.ts`**: Added `estimateStorage()` static method; auto-unload on fatal WASM errors (Tokenizer/deleted object/disposed) in `generate()`
2. **`src/lib/loop-controller.ts`**: Added `clearError()` public method; added `friendlyError()` private helper translating quota/cache/tokenizer/network errors to user-friendly messages; all 3 `onError` callbacks now use `friendlyError()`; LISTENING stage entry clears `state.error`
3. **`src/components/model/model-selector.tsx`**: Added storage estimation hook; shows "low storage" badge on models exceeding available storage
4. **`src/App.tsx`**: Error display now has dismiss button (×) calling `controller.clearError()`
5. **`src/lib/__tests__/llm-engine.test.ts`**: 5 new tests (auto-unload Tokenizer, auto-unload disposed, no auto-unload on non-fatal, estimateStorage available, estimateStorage unavailable)
6. **`src/lib/__tests__/loop-controller.test.ts`**: 5 new tests (clearError, clearError notifies, friendlyError quota, friendlyError tokenizer, friendlyError passthrough)

## Decisions Made

- **Auto-unload on fatal errors**: Chose to null out engine+modelId directly (no async unload call) since the WASM is already corrupted. This makes `isLoaded()` return false immediately so FSM falls back to rule-based.
- **friendlyError in controller, not engine**: Error translation lives in the controller because it has context about user actions. The engine reports raw errors.
- **Clear error on LISTENING only**: Not on every stage transition, to avoid clearing errors too aggressively during error recovery.

## Open Questions

- None

## Links

Commits:
- (to be added)

=== S-2026-02-08-1700-real-search-proxy ===
# Session

Session-ID: S-2026-02-08-1700-real-search-proxy
Title: Replace mock search with real web search via Lambda proxy
Date: 2026-02-08
Author: Claude

## Goal

Replace MockSearchProvider with real web search via AWS Lambda proxy. Cascade Google Custom Search (100 free/day) → Brave (2K free/month) → Tavily (1K free/month). Add quota tracking UI panel.

## Context

All search infrastructure exists (interface, detection, formatting, FSM integration, UI). Only missing piece is a real search API provider. Browser can't call search APIs directly due to CORS + API key security.

## Plan

1. Lambda function with cascading provider logic + DynamoDB quota tracking
2. Deploy script for Lambda + API Gateway + DynamoDB
3. Browser-side ProxySearchProvider
4. Quota tracking UI panel
5. Wire into app via build-time URL variable

## Changes Made

(to be filled)

## Decisions Made

(to be filled)

## Open Questions

- Need to sign up for Google CSE, Brave, and Tavily API keys

## Links

Commits:
- (to be added)

=== S-2026-02-08-2000-changelog-qa-rag ===
# Session S-2026-02-08-2000-changelog-qa-rag

Session-ID: S-2026-02-08-2000-changelog-qa-rag
Title: Add Q&A to changelog via BroadcastChannel and RAG
Date: 2026-02-08
Author: dmar

## Goal

Add Q&A functionality to Changelog using BroadcastChannel + RAG pattern

## Context

The Changelog window displays all coding sessions but couldn't answer questions about them. Users wanted to ask questions like "Why did we add audio mute?" and get answers from the LLM using actual session content.

**Problem:**
- Changelog opens in separate window (can't access main app's LLM)
- Loading LLM twice would waste 2GB+ memory
- Need accurate answers with citations, not hallucinations

**Solution:**
- Use BroadcastChannel API for cross-window communication
- Implement RAG (Retrieval Augmented Generation) pattern:
  1. Search keyword index first (fast)
  2. Find top 5 relevant sessions
  3. Read full session content from disk
  4. LLM generates answer with session context

## Plan

1. Add BroadcastChannel listener to main app (App.tsx)
2. Add Q&A UI to Changelog window (changelog-content.ts)
3. Implement keyword search + LLM generation
4. Add markdown rendering for formatted output

## Changes Made

1. **`src/App.tsx`**: Added BroadcastChannel('llm-service') listener
   - Receives query + sessionIds from Changelog window
   - Reads session markdown files via fetch()
   - Uses `controller.getLLMEngine().generate()` with session context
   - Returns answer via BroadcastChannel
   - 30 second timeout handling

2. **`src/components/changelog/changelog-content.ts`**: Added Q&A UI
   - Input box, Ask button, status display, answer area
   - Searches keyword index to find relevant sessions (top 5)
   - Sends sessionIds via BroadcastChannel (efficient - not full content)
   - Renders markdown answers with marked.js
   - Enter key support, loading states, error handling

3. **System prompt updates**: Encourages markdown formatting
   - Headings, bullet points, code blocks
   - Bold Session ID citations
   - Structured, scannable output

4. **Bug fixes**:
   - Increased maxTokens from 256 to 512 (answers were truncated)
   - Strip `<think>` tags from LLM output (internal reasoning shouldn't show)

5. **Markdown rendering**:
   - Added marked.js v11 via CDN
   - Comprehensive CSS styling for headings, lists, code, quotes
   - Professional documentation-style output

## Decisions Made

- **BroadcastChannel over Shared Worker**: Simpler API, same origin only (acceptable)
- **RAG pattern**: Search first, then generate (accurate answers with citations vs blind LLM guessing)
- **Keyword index for search**: Fast, existing infrastructure, scales well
- **Top 5 sessions**: Balance between context and token limit
- **Markdown rendering**: Better UX than plain text, easy to scan
- **marked.js library**: Lightweight, popular, simple API

## Testing

Tested locally and on CloudFront:
1. Load model in main app
2. Open Changelog
3. Ask: "Why did we add audio mute?" → Cites S-2026-02-08-1400-listener-ui-mute
4. Ask: "Summarize the changes" → Lists recent sessions with details
5. Verify markdown rendering (headings, lists, code blocks)

## Open Questions

- None - feature complete and deployed

## Links

- PR/Commits:
  - `469a683` - Add BroadcastChannel Q&A to Changelog
  - `24555a4` - Fix Q&A truncation and thinking tags
  - `ec1fb52` - Add markdown rendering to Q&A answers
- Deployed: https://dmpt2ecfvyptf.cloudfront.net
- Related: S-2026-02-08-1430-migrate-project-memory (initial memory system migration)

=== S-2026-02-09-0040-mobile-boot-greeting ===
# Session

Session-ID: S-2026-02-09-0040-mobile-boot-greeting
Title: Add mobile voice greeting and fix model load race condition
Date: 2026-02-09
Author: Claude

## Goal

Add a voice welcome greeting on mobile during model loading, and fix a race condition in model load/unload that caused models to fail loading after pressing X.

## Context

On mobile (iPhone/Android), the app auto-loads an LLM model on startup which takes several seconds. During this time the user stares at a loading bar with no feedback. Also, a race condition in LLMEngine meant pressing X (unload) while a model was loading could corrupt state, preventing subsequent loads.

## Plan

1. Add native SpeechSynthesis greeting on mobile boot ("Well, hello there!")
2. Play ding sounds every 2s while model loads
3. Play happy jingle + say "Ok ready" when model finishes loading
4. Fix iOS audio by adding tap-to-start splash (iOS requires user gesture for audio)
5. Fix load/unload race condition with operation queue + cancellation token
6. Add comprehensive test cases for race conditions

## Changes Made

### `src/lib/ding-tone.ts` (new file export)
- Added `generateHappyJingleBlob()` — ascending C major arpeggio (C5-E5-G5-C6) with shimmer harmonics

### `src/App.tsx`
- Added `mobileTapped` state for tap-to-start splash screen (unlocks iOS audio)
- Added boot greeting flow: SpeechSynthesis greetings → ding interval → happy jingle on load
- Splash screen with Bug Loop branding and "Tap anywhere to start" prompt

### `src/lib/llm-engine.ts`
- Added `opQueue` promise chain to serialize all load/unload operations
- Added `loadToken` cancellation mechanism — created synchronously in `loadModel()`, cancelled synchronously in `unloadModel()`
- `doLoad()` checks token at each await point, tears down partially-created engines on cancellation
- Throws "Model load cancelled" on cancellation (not treated as an error in UI)

### `src/lib/__tests__/llm-engine.test.ts` (10 new tests)
- Unload resets isLoading, notifies completion
- Reload same/different model after unload
- Unload during load cancels the load
- Load after unload-during-load succeeds cleanly
- Rapid double-unload safety
- Rapid load-load serialization (last model wins)
- Unload clears loading flag mid-load

### `src/lib/__tests__/loop-controller.test.ts` (5 new tests)
- Unload resets loadProgress, notifies listeners
- Reload same model after unload
- Unload during load cancels cleanly
- Rapid double-unload safety

## Decisions Made

- **Native SpeechSynthesis over VITS**: Works without user gesture on most browsers (except iOS), available immediately without model download
- **Tap-to-start splash for iOS**: iOS requires user gesture for all audio APIs. Splash screen serves as UX welcome + audio unlock
- **Cancellation token over generation counter**: Generation counter fails when load/unload are queued synchronously before microtasks run (counter is already bumped by the time doLoad captures it). Token is created in loadModel and cancelled in unloadModel, both synchronously.
- **Operation queue serialization**: Simple promise chaining ensures load/unload never overlap, preventing state corruption

## Open Questions

- Ding sounds via Audio element may be silenced by iOS auto-play policy even after tap (SpeechSynthesis is primary UX, dings are secondary)
- Should we add a cancellation UI indicator ("Loading cancelled")?

## Links

Commits:
- (pending)

=== S-2026-02-11-0010-auto-index-hooks ===
# Session

Session-ID: S-2026-02-11-0010-auto-index-hooks
Title: Integrate bash auto-index system with git hooks
Date: 2026-02-11
Author: dmar

## Goal

Integrate the bash-based auto-index system from the traceable-searchable-adr-memory-index template repo so the project memory index rebuilds automatically on every commit.

## Context

The project had TypeScript-based index scripts (`build-index.ts`, `search-memory.ts`) but no git hooks installed. The index was stale and had to be rebuilt manually. The changelog UI was reading from `.index/metadata.json` and `.index/keywords.json` but these weren't being kept up to date.

## Plan

1. Copy bash scripts (build-index.sh, setup-hooks.sh, test.sh) from template repo
2. Install pre-commit hook via setup-hooks.sh
3. Build initial index
4. Fix format mismatch — bash builder must produce the same JSON format as the TS builder (changelog UI depends on it)

## Changes Made

- Added `scripts/build-index.sh` — rewrote to produce TS-compatible format:
  - `metadata.json`: array of `{sessionId, file, date, author, goal, keywords}` objects
  - `keywords.json`: `{keyword: [sessionId, ...]}` map for search
  - `sessions.txt`: plain text concatenation for grep
  - `last-updated.txt`: build timestamp
- Added `scripts/setup-hooks.sh` — installs pre-commit hook
- Added `scripts/test.sh` — test runner
- Added `tests/test-index-builder.sh` — test suite for index builder
- Installed `.git/hooks/pre-commit` — auto-rebuilds index and stages files before each commit

## Decisions Made

- **Bash build script instead of TS-only**: The bash version has no runtime dependencies beyond `jq`, works on any Unix system, and is simpler to run from a git hook. The TS version (`build-index.ts`) is kept as an alternative.
- **Match TS output format**: The changelog UI (`changelog-content.ts`) imports `metadata.json` and `keywords.json` at build time. The bash script must produce the same schema or the changelog breaks (which it did initially).
- **Stop word filtering in bash**: Ported the same stop word list from `build-index.ts` to keep keyword quality consistent.

## Open Questions

None.

## Links

Commits:
- (pending)

PRs:
- N/A

ADRs:
- N/A

